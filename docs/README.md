# Dictado Inteligente con Whisper (local) + post-procesado opcional con LLM

## Qu√© hace
1. **Escucha tu micro**, detecta autom√°ticamente cu√°ndo hablas (VAD) y corta por frases.
2. **Transcribe localmente** con [faster-whisper] (no env√≠a audio a Internet).
3. **(Opcional) Mejora el texto** con un LLM (p. ej., `gpt-4o-mini`) para:
   - limpiar y puntuar mejor,
   - resumir,
   - sacar bullets/tareas,
   - formatear como email o acta de reuni√≥n.

## Instalaci√≥n (Windows/macOS/Linux)
1. **Instala FFmpeg** (necesario para mejor compatibilidad de audio).
   - macOS: `brew install ffmpeg`
   - Ubuntu/Debian: `sudo apt-get install ffmpeg`
   - Windows: descarga binarios desde la web oficial y a√±ade a PATH.
2. Crea y activa un entorno:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Windows: .venv\\Scripts\\activate
   ```
3. Instala dependencias:
   ```bash
   pip install -r requirements.txt
   ```

## Uso b√°sico
```bash
python whisper_dictado.py
```

## Configuraci√≥n avanzada
1. Copia `config_template.env` a `.env` y ajusta par√°metros:
   ```bash
   cp config_template.env .env
   ```
2. Edita `.env` para personalizar:
   - Modelo de Whisper (tiny, base, small, medium, large)
   - Idioma (es, en, auto)
   - Configuraci√≥n de VAD
   - **LLM Provider**: `openrouter` (recomendado) o `openai`
   - **API Key**: De OpenRouter o OpenAI
   - **Modelo LLM**: `openai/gpt-4o-mini`, `anthropic/claude-3-haiku`, etc.

## Caracter√≠sticas principales

### üé§ Detecci√≥n autom√°tica de voz (VAD)
- Detecta cuando empiezas y terminas de hablar
- Evita transcribir silencios innecesarios
- Configurable por sensibilidad

### üß† Transcripci√≥n local con Whisper
- **Sin conexi√≥n a Internet** para la transcripci√≥n
- M√∫ltiples modelos disponibles (tiny ‚Üí large)
- Soporte para m√∫ltiples idiomas
- Correcci√≥n autom√°tica de puntuaci√≥n b√°sica

### ‚ú® Post-procesado inteligente (opcional)
- Mejora la puntuaci√≥n y formato
- Extrae tareas y bullets autom√°ticamente
- Formatea como emails o actas de reuni√≥n
- Res√∫menes inteligentes
- **Soporte para m√∫ltiples proveedores de LLM:**
  - **OpenRouter** (recomendado): Acceso a m√∫ltiples modelos (GPT-4, Claude, Llama, etc.)
  - **OpenAI**: Modelos oficiales de OpenAI

### üìÅ Gesti√≥n de archivos
- Guarda autom√°ticamente transcripciones
- Historial de sesiones
- Exportaci√≥n a m√∫ltiples formatos

## Ejemplos de uso

### Dictado b√°sico
```bash
python whisper_dictado.py
```

### Con post-procesado LLM
```bash
python whisper_dictado.py --llm-enable
```

### Configurar OpenRouter (Recomendado)
1. Ve a [OpenRouter](https://openrouter.ai/keys) y crea una cuenta
2. Obt√©n tu API key
3. Configura en `.env`:
   ```env
   LLM_ENABLED=true
   LLM_PROVIDER=openrouter
   OPENAI_API_KEY=tu_api_key_aqui
   OPENAI_MODEL=openai/gpt-4o-mini
   ```
4. Prueba la conexi√≥n:
   ```bash
   python test_openrouter.py
   ```

### Idioma espec√≠fico
```bash
python whisper_dictado.py --idioma es
```

### Modelo espec√≠fico
```bash
python whisper_dictado.py --modelo small
```

## Estructura del proyecto
```
whisper_inteligente/
‚îú‚îÄ‚îÄ whisper_dictado.py      # Script principal
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ config_template.env # Plantilla de configuraci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ config.py          # M√≥dulo de configuraci√≥n
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ audio_handler.py   # Manejo de audio
‚îÇ   ‚îú‚îÄ‚îÄ vad_detector.py    # Detecci√≥n de voz
‚îÇ   ‚îî‚îÄ‚îÄ text_processor.py  # Procesamiento de texto
‚îú‚îÄ‚îÄ output/                # Transcripciones guardadas
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias
‚îî‚îÄ‚îÄ README.md             # Este archivo
```

## Troubleshooting

### Error de audio
- Verifica que tu micr√≥fono est√© conectado
- En Windows, ejecuta como administrador si hay problemas de permisos
- Prueba diferentes dispositivos de audio en la configuraci√≥n

### Error de FFmpeg
- Aseg√∫rate de que FFmpeg est√© instalado y en el PATH
- Reinicia la terminal despu√©s de instalar FFmpeg

### Problemas de transcripci√≥n
- Prueba con un modelo m√°s grande (small, medium, large)
- Verifica que el idioma est√© configurado correctamente
- Habla m√°s claro y cerca del micr√≥fono

## Contribuir
¬°Las contribuciones son bienvenidas! Por favor:
1. Fork el proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

## Licencia
MIT License - ver LICENSE para m√°s detalles.
