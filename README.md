# ğŸ™ï¸ AudioLetra - Captura tus ideas, la IA las escribe

Una aplicaciÃ³n web moderna para transcripciÃ³n de audio usando Whisper y procesamiento inteligente con LLM.

## âœ¨ CaracterÃ­sticas

- ğŸ¤ **GrabaciÃ³n de audio en tiempo real** con detecciÃ³n de voz
- ğŸ§  **TranscripciÃ³n con Whisper** (modelo local)
- ğŸ¤– **Procesamiento inteligente** con LLM (OpenAI/OpenRouter)
- ğŸŒ™ **Modo oscuro/claro** con transiciones suaves
- ğŸ“± **DiseÃ±o responsivo** optimizado para mÃ³viles
- ğŸ¨ **Interfaz moderna** con animaciones elegantes
- ğŸ“Š **GestiÃ³n de dictados** con historial
- ğŸŒ **TraducciÃ³n automÃ¡tica** al inglÃ©s
- ğŸ“ **ResÃºmenes inteligentes** del contenido

## ğŸš€ Inicio RÃ¡pido

### InstalaciÃ³n

```bash
# Clonar el repositorio
git clone <repository-url>
cd AudioLetra

# Crear entorno virtual
python -m venv .venv

# Activar entorno virtual
# Windows:
.venv\Scripts\activate
# Linux/macOS:
source .venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# ConfiguraciÃ³n inicial
python install.py
```

### ConfiguraciÃ³n

1. **Configurar variables de entorno**:
   ```bash
   cp config/config_template.env .env
   # Editar .env con tus configuraciones
   ```

2. **Configurar LLM (opcional)**:
   ```bash
   python configure_openrouter.py
   ```

### Uso

#### Interfaz Web (Recomendado)
```bash
python web_server.py --host 127.0.0.1 --port 5000
```
Abre tu navegador en `http://127.0.0.1:5000`

#### LÃ­nea de Comandos
```bash
python whisper_dictado.py [--llm-enable]
```

## ğŸ“ Estructura del Proyecto

```
AudioLetra/
â”œâ”€â”€ ğŸ“ config/           # Configuraciones
â”‚   â”œâ”€â”€ config_template.env
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ ğŸ“ data/             # Datos del proyecto
â”‚   â”œâ”€â”€ audio/           # Archivos de audio
â”‚   â”œâ”€â”€ transcriptions/  # Transcripciones
â”‚   â””â”€â”€ models/          # Modelos de IA
â”œâ”€â”€ ğŸ“ docs/             # DocumentaciÃ³n
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ WEB_README.md
â”‚   â””â”€â”€ AGENTS.md
â”œâ”€â”€ ğŸ“ scripts/          # Scripts auxiliares
â”œâ”€â”€ ğŸ“ tests/            # Pruebas
â”œâ”€â”€ ğŸ“ utils/            # Utilidades
â”‚   â”œâ”€â”€ audio_handler.py
â”‚   â”œâ”€â”€ text_processor.py
â”‚   â””â”€â”€ vad_detector.py
â”œâ”€â”€ ğŸ“ web/              # Interfaz web
â”‚   â”œâ”€â”€ static/          # CSS, JS, imÃ¡genes
â”‚   â””â”€â”€ templates/       # Plantillas HTML
â”œâ”€â”€ ğŸ“„ web_server.py     # Servidor web principal
â”œâ”€â”€ ğŸ“„ whisper_dictado.py # CLI principal
â””â”€â”€ ğŸ“„ install.py        # Instalador
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

Crea un archivo `.env` basado en `config/config_template.env`:

```env
# Whisper
WHISPER_MODEL=base
WHISPER_LANGUAGE=es

# LLM (OpenAI/OpenRouter)
OPENAI_API_KEY=tu_api_key
OPENAI_MODEL=gpt-3.5-turbo
LLM_PROVIDER=openai
LLM_BASE_URL=https://api.openai.com/v1

# Audio
SAMPLE_RATE=16000
CHUNK_SIZE=1024

# Rutas
OUTPUT_DIR=output
```

### Modelos Whisper Disponibles

- `tiny` - MÃ¡s rÃ¡pido, menos preciso
- `base` - Equilibrio velocidad/precisiÃ³n (recomendado)
- `small` - Mejor precisiÃ³n
- `medium` - Alta precisiÃ³n
- `large` - MÃ¡xima precisiÃ³n (requiere mÃ¡s recursos)

## ğŸ§ª Pruebas

```bash
# Prueba de configuraciÃ³n
python tests/test_setup.py

# Prueba del servidor web
python tests/test_web_server.py

# Prueba de configuraciÃ³n LLM
python tests/test_openrouter.py
```

## ğŸ› ï¸ Desarrollo

### Estructura de Desarrollo

- **Frontend**: HTML5, CSS3, JavaScript vanilla
- **Backend**: Python Flask
- **IA**: Whisper (OpenAI), LLM (OpenAI/OpenRouter)
- **Audio**: PyAudio, NumPy

### Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -m 'AÃ±adir nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“‹ Requisitos

- Python 3.8+
- MicrÃ³fono funcional
- ConexiÃ³n a internet (para LLM)
- 4GB RAM mÃ­nimo (8GB recomendado para modelos grandes)

## ğŸ”’ Seguridad

- âš ï¸ **Nunca commits archivos `.env`** con API keys
- ğŸ” Las API keys se almacenan localmente
- ğŸŒ El audio se procesa localmente (Whisper)
- â˜ï¸ Solo el texto se envÃ­a al LLM (opcional)

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ¤ Soporte

Si tienes problemas o preguntas:

1. Revisa la documentaciÃ³n en `docs/`
2. Ejecuta las pruebas para diagnosticar problemas
3. Abre un issue en GitHub
4. Contacta al desarrollador

## ğŸ¯ Roadmap

- [ ] Soporte para mÃ¡s idiomas
- [ ] IntegraciÃ³n con mÃ¡s proveedores LLM
- [ ] ExportaciÃ³n a diferentes formatos
- [ ] API REST para integraciÃ³n
- [ ] Modo offline completo
- [ ] Reconocimiento de voz en tiempo real

---

**Desarrollado con â¤ï¸ por Alberto Sanahuja**
